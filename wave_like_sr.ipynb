{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "thesis_code.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-42A23jxCdMB"
      },
      "source": [
        "# Author: Gergő Gömöri\n",
        "# Email: gomorigergo91@gmail.com\n",
        "\n",
        "# Short description: In a grid-world, a rodent moves around in a room looking for some delicious juice.\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import random\n",
        "from IPython import display\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import networkx as nx\n",
        "\n",
        "# Colours\n",
        "WHITE  = '#FFFFFF'   # empty cell\n",
        "GRAY   = '#5F5F60'   # mouse\n",
        "ORANGE = '#F2AA4C'   # juice\n",
        "DARK   = '#2A3132'   # obstacle\n",
        "GREEN  = '#ADEFD1FF' # start\n",
        "BLUE   = '#ADD8E6'   # decision point\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mc3o_MqKCnlH"
      },
      "source": [
        "def draw_maze(maze):\n",
        "\n",
        "    col_map = {0: WHITE, 1: ORANGE, 2: DARK, 3: GREEN, 4: BLUE}\n",
        "\n",
        "    rows,cols    = maze.shape\n",
        "    colored_maze = [[col_map[maze[j,i]] for i in range(cols)] for j in range(rows)]\n",
        "\n",
        "    fig = plt.figure(1, figsize=(cols,rows))\n",
        "\n",
        "    ax = plt.gca()\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "\n",
        "    rows,cols    = maze.shape\n",
        "    colored_maze = [[col_map[maze[j,i]] for i in range(cols)] for j in range(rows)]\n",
        "\n",
        "    fig = plt.figure(1, figsize=(cols,rows))\n",
        "\n",
        "    grid = plt.table(cellText=None,\n",
        "                            cellColours=colored_maze,\n",
        "                            cellLoc='center',\n",
        "                            loc=(0,0),\n",
        "                            edges='closed')\n",
        "    tc = grid.properties()['children']\n",
        "    for cell in tc:\n",
        "        cell.set_height(1.0/rows)\n",
        "        cell.set_width(1.0/cols)\n",
        "\n",
        "\n",
        "# Convention:\n",
        "# 0 = empty\n",
        "# 1 = orange juice\n",
        "# 2 = obstacle\n",
        "# 3 = start\n",
        "# 4 = decision point\n",
        "\n",
        "maze_small = np.array([\n",
        "    [3, 0, 0, 0],\n",
        "    [0, 0, 0, 0],\n",
        "    [0, 0, 0, 0],\n",
        "    [0, 0, 0, 1]\n",
        "])\n",
        "draw_maze(maze_small)\n",
        "plt.savefig(\"maze_small\", dpi = 300)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "maze_default = np.array([\n",
        "    [3, 0, 0, 0, 0, 0, 0, 0],\n",
        "    [0, 0, 0, 0, 0, 0, 0, 0],\n",
        "    [0, 0, 0, 0, 0, 0, 0, 0],\n",
        "    [0, 0, 0, 0, 0, 0, 0, 0],\n",
        "    [0, 0, 0, 0, 0, 0, 0, 0],\n",
        "    [0, 0, 0, 0, 0, 0, 0, 0],\n",
        "    [0, 0, 0, 0, 0, 0, 0, 0],\n",
        "    [0, 0, 0, 0, 0, 0, 0, 1]\n",
        "])\n",
        "draw_maze(maze_default)\n",
        "plt.savefig(\"maze_default\", dpi = 300)\n",
        "plt.show()\n",
        "\n",
        "maze_new_rew = np.array([\n",
        "    [3, 0, 0, 0, 0, 0, 0, 0],\n",
        "    [0, 0, 0, 0, 0, 0, 0, 0],\n",
        "    [0, 0, 0, 0, 0, 0, 0, 0],\n",
        "    [0, 0, 0, 0, 0, 0, 0, 0],\n",
        "    [0, 0, 0, 0, 0, 0, 0, 0],\n",
        "    [0, 0, 0, 0, 0, 0, 0, 0],\n",
        "    [0, 0, 0, 0, 0, 0, 0, 0],\n",
        "    [1, 0, 0, 0, 0, 0, 0, 0]\n",
        "])\n",
        "draw_maze(maze_new_rew)\n",
        "plt.savefig(\"maze_new_rew\", dpi = 300)\n",
        "plt.show()\n",
        "\n",
        "maze_labyrinth = np.array([\n",
        "    [0, 4, 0, 2, 2, 2, 0, 4, 0],\n",
        "    [2, 0, 2, 2, 2, 2, 2, 0, 2],\n",
        "    [2, 4, 0, 0, 4, 0, 0, 4, 2],\n",
        "    [2, 0, 2, 2, 0, 2, 2, 0, 2],\n",
        "    [0, 4, 0, 2, 0, 2, 0, 4, 0],\n",
        "    [2, 2, 2, 2, 0, 2, 2, 2, 2],\n",
        "    [3, 0, 0, 0, 4, 2, 2, 2, 2],\n",
        "    [2, 2, 2, 2, 0, 2, 2, 2, 2],\n",
        "    [0, 4, 0, 2, 0, 2, 1, 4, 0],\n",
        "    [2, 0, 2, 2, 0, 2, 2, 0, 2],\n",
        "    [2, 4, 0, 0, 4, 0, 0, 4, 2],\n",
        "    [2, 0, 2, 2, 2, 2, 2, 0, 2],\n",
        "    [0, 4, 0, 2, 2, 2, 0, 4, 0]\n",
        "])\n",
        "draw_maze(maze_labyrinth)\n",
        "plt.savefig(\"maze_labyrinth\", dpi = 300)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wa9ZfVQM3laI"
      },
      "source": [
        "class Maze:\n",
        "    def __init__(self, maze):\n",
        "        self.maze                     = maze\n",
        "        self.actions                  = self.actions()\n",
        "        self.states, self.map         = self.states()\n",
        "        self.start_state              = self.get_start()\n",
        "        self.terminal_state           = self.get_terminal()\n",
        "        self.n_actions                = len(self.actions)\n",
        "        self.n_states                 = len(self.states)\n",
        "        self.rewards                  = self.rewards()\n",
        "\n",
        "    def actions(self):\n",
        "        actions = dict()\n",
        "        actions[0] = ( 0,-1) # LEFT\n",
        "        actions[1] = ( 0, 1) # RIGHT\n",
        "        actions[2] = (-1, 0) # UP\n",
        "        actions[3] = ( 1, 0) # DOWN\n",
        "        return actions\n",
        "\n",
        "    # States are (x_mouse, y_mouse)\n",
        "    def states(self):\n",
        "        states = dict()\n",
        "        map = dict()\n",
        "        s = 0\n",
        "        for i in range(self.maze.shape[0]):\n",
        "            for j in range(self.maze.shape[1]):\n",
        "                states[s] = (i, j)\n",
        "                map[(i, j)] = s\n",
        "                s += 1\n",
        "        return states, map\n",
        "\n",
        "    def get_terminal(self):\n",
        "        for i in range(self.maze.shape[0]):\n",
        "            for j in range(self.maze.shape[1]):\n",
        "                if self.maze[i, j] == 1:\n",
        "                    terminal_state = self.map[(i, j)]\n",
        "        return terminal_state\n",
        "\n",
        "    def get_start(self):\n",
        "        for i in range(self.maze.shape[0]):\n",
        "            for j in range(self.maze.shape[1]):\n",
        "                if self.maze[i, j] == 3:\n",
        "                    start = self.map[(i, j)]\n",
        "        return start\n",
        "\n",
        "    def possible_actions(self, state):\n",
        "        # In total 8 special cases plus check for obstacles\n",
        "        (x, y) = self.states[state]\n",
        "        poss_actions = np.array([0, 1, 2, 3])\n",
        "\n",
        "        # Top left corner\n",
        "        if x == 0 and y == 0:\n",
        "            poss_actions = poss_actions[np.all([poss_actions != 0, poss_actions != 2], axis = 0)]\n",
        "        \n",
        "        # Bottom left corner\n",
        "        if x == (self.maze.shape[0] - 1) and y == 0:\n",
        "            poss_actions = poss_actions[np.all([poss_actions != 0, poss_actions != 3], axis = 0)]\n",
        "        \n",
        "        # Top right corner\n",
        "        if x == 0 and y == (self.maze.shape[1] - 1):\n",
        "            poss_actions = poss_actions[np.all([poss_actions != 1, poss_actions != 2], axis = 0)]\n",
        "\n",
        "        # Bottom right corner\n",
        "        if x == (self.maze.shape[0] - 1) and y == (self.maze.shape[1] - 1):\n",
        "            poss_actions = poss_actions[np.all([poss_actions != 1, poss_actions != 3], axis = 0)]\n",
        "\n",
        "        # Left edge\n",
        "        if y == 0:\n",
        "            poss_actions = poss_actions[poss_actions != 0]\n",
        "\n",
        "        # Right edge\n",
        "        if y == (self.maze.shape[1] - 1):\n",
        "            poss_actions = poss_actions[poss_actions != 1]\n",
        "\n",
        "        # Upper edge\n",
        "        if x == 0:\n",
        "            poss_actions = poss_actions[poss_actions != 2]\n",
        "\n",
        "        # Bottom edge\n",
        "        if x == (self.maze.shape[0] - 1):\n",
        "            poss_actions = poss_actions[poss_actions != 3]\n",
        "\n",
        "        # Check obstacle in each direction\n",
        "        # To the left\n",
        "        if y > 0:\n",
        "            if self.maze[x, y - 1] == 2:\n",
        "                poss_actions = poss_actions[poss_actions != 0]\n",
        "\n",
        "        # To the right\n",
        "        if y != (self.maze.shape[1] - 1): \n",
        "            if self.maze[x, y + 1] == 2:\n",
        "                poss_actions = poss_actions[poss_actions != 1]\n",
        "\n",
        "        # Upwards\n",
        "        if x > 0:\n",
        "            if self.maze[x - 1, y] == 2:\n",
        "                poss_actions = poss_actions[poss_actions != 2]\n",
        "\n",
        "        # Downwards\n",
        "        if x != (self.maze.shape[0] - 1):\n",
        "            if self.maze[x + 1, y] == 2:\n",
        "                poss_actions = poss_actions[poss_actions != 3]\n",
        "        \n",
        "        # No actions possible from walls\n",
        "        if self.maze[x, y] == 2:\n",
        "            poss_actions = []\n",
        "\n",
        "        return poss_actions\n",
        "\n",
        "    def move(self, state, action):\n",
        "        # Compute the future position\n",
        "        row = self.states[state][0] + self.actions[action][0]\n",
        "        col = self.states[state][1] + self.actions[action][1]\n",
        "        return self.map[(row, col)]\n",
        "\n",
        "    def rewards(self):\n",
        "        rewards = np.full((self.n_states, self.n_actions), np.NINF)\n",
        "\n",
        "        # Reward is only obtained from terminal state\n",
        "        for s in range(self.n_states):\n",
        "            for a in self.possible_actions(s):\n",
        "                if self.move(s, a) == self.terminal_state:\n",
        "                    rewards[s, a] = 1\n",
        "                else:\n",
        "                    rewards[s, a] = 0\n",
        "        return rewards\n",
        "\n",
        "    def simulate(self, start, policy):\n",
        "        path = []\n",
        "        counter = 0\n",
        "        path.append(self.states[start])\n",
        "        s = start\n",
        "        \n",
        "        while s != self.terminal_state:\n",
        "            if counter == 25 or (policy[s] not in self.possible_actions(s)):\n",
        "                path = []\n",
        "                break\n",
        "\n",
        "            next_s = self.move(s, policy[s])\n",
        "            path.append(self.states[next_s])\n",
        "            s = next_s\n",
        "            counter += 1\n",
        "\n",
        "        if path == []:\n",
        "            total_steps = np.inf\n",
        "        else:\n",
        "            total_steps = len(path) - 1\n",
        "        return path, total_steps\n",
        "\n",
        "    def show(self):\n",
        "        print('The states are :')\n",
        "        print(self.states)\n",
        "        print('The mapping of the states:')\n",
        "        print(self.map)\n",
        "        print('The start state is:')\n",
        "        print(self.start_state)\n",
        "        print('The terminal state is:')\n",
        "        print(self.terminal_state)\n",
        "        print(self.states[self.terminal_state])\n",
        "        print('The actions are:')\n",
        "        print(self.actions)\n",
        "        print('The rewards are:')\n",
        "        print(self.rewards)\n",
        "\n",
        "\n",
        "# Description of the environment\n",
        "env = Maze(maze_default)\n",
        "env.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ka82zGWVjzrx"
      },
      "source": [
        "def create_map(successor_representation, target, env):\n",
        "\n",
        "    # Initialization\n",
        "    map = []\n",
        "    buffer = []\n",
        "    nodes = []\n",
        "\n",
        "    # Add first node\n",
        "    buffer.append([target])\n",
        "    nodes.append(target)\n",
        "\n",
        "    for i in range(1000000):\n",
        "        batch = []\n",
        "        next_nodes = []\n",
        "        for j in range(len(buffer[0])):\n",
        "\n",
        "            selected = find_best_neighbours(successor_representation, buffer[0][j], env)\n",
        "\n",
        "            if selected.shape == (0,):\n",
        "                continue\n",
        "\n",
        "            candidate_nodes = selected[:, 2].astype(int)\n",
        "\n",
        "            for m in range(len(candidate_nodes)):\n",
        "                if candidate_nodes[m] not in nodes:\n",
        "                    batch.append(selected[m])\n",
        "                    next_nodes.append(candidate_nodes[m])\n",
        "                    nodes.append(candidate_nodes[m])\n",
        "\n",
        "        if next_nodes == []:\n",
        "            break\n",
        "        else:\n",
        "            map.append(batch)\n",
        "            buffer.append(next_nodes)\n",
        "            del buffer[0]\n",
        "\n",
        "    return map\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UCYtJxycau_"
      },
      "source": [
        "def probably_node_leads_to(successor_representation, node, env):\n",
        "\n",
        "    M_row = successor_representation[node, :]\n",
        "    results = []\n",
        "\n",
        "    for i in range(0, len(M_row), env.n_actions):\n",
        "        segment = M_row[i : i + env.n_actions]\n",
        "        sum = np.sum(segment)\n",
        "        divisor = len(segment)\n",
        "        for ind, val in enumerate(segment):\n",
        "            if val == 0:\n",
        "                divisor -= 1\n",
        "        if divisor == 0:\n",
        "            sum = 0\n",
        "        else:\n",
        "            sum /= divisor\n",
        "        results.append(sum)\n",
        "\n",
        "    return np.argsort(results)[-2]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XI3AQqZhfyyM"
      },
      "source": [
        "def find_best_neighbours(successor_representation, target, env):\n",
        "\n",
        "    # 1. A list is returned with elements [target, value_in_M, ID]\n",
        "    neighbours = np.array([[target, successor_representation[i, target], i] for i in range(len(successor_representation[:, target])) if ((successor_representation[i, target] != 0) and (i != target))])\n",
        "\n",
        "    if neighbours.shape == (0,):\n",
        "        return neighbours\n",
        "\n",
        "    # 2. Find out which state does the target most possibly lead to\n",
        "    leads_to_state = probably_node_leads_to(successor_representation, target, env)\n",
        "\n",
        "    # 3. Remove the nodes which are initiated at the state where this node leads to (these nodes just lead back)\n",
        "\n",
        "    neighbour_candidate_states = np.floor(neighbours[:, 2] / env.n_actions)\n",
        "\n",
        "    ind_leading_back = []\n",
        "\n",
        "    for ind, state in enumerate(neighbour_candidate_states):\n",
        "        if state == leads_to_state:\n",
        "            ind_leading_back.append(ind)\n",
        "\n",
        "    neighbours = np.delete(neighbours, ind_leading_back, axis = 0)\n",
        "\n",
        "    # 4. Remove the neighbours which do not lead to the state of the target\n",
        "\n",
        "    target_state = np.floor(target / env.n_actions)\n",
        "    leads_elsewhere = []\n",
        "\n",
        "    for ind, cand_neigh in enumerate(neighbours):\n",
        "        current_node = int(cand_neigh[2])\n",
        "        current_node_leads_to = probably_node_leads_to(successor_representation, current_node, env)\n",
        "        if current_node_leads_to != target_state:\n",
        "            leads_elsewhere.append(ind)\n",
        "\n",
        "    neighbours = np.delete(neighbours, leads_elsewhere, axis = 0)\n",
        "\n",
        "    return neighbours\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMn9OYMeYQd7"
      },
      "source": [
        "def update_value_weights_grad(env, successor_representation, weights, target, reward, map, learning_rate_wave, discount_factor_wave):\n",
        "    # Parameters\n",
        "    # - learning_rate_wave\n",
        "    # - discount_factor_wave\n",
        "\n",
        "    # Initialization\n",
        "    params = weights\n",
        "    dopamine = {}\n",
        "\n",
        "    # Extract nodes from the map\n",
        "    nodes = []\n",
        "    nodes.append([target])\n",
        "    for i in range(len(map)):\n",
        "        batch = []\n",
        "        for j in range(len(map[i])):\n",
        "            batch.append(int(map[i][j][2]))\n",
        "        nodes.append(batch)\n",
        "\n",
        "\n",
        "    # Looping through rings\n",
        "    for k, subset in enumerate(nodes):\n",
        "        # Looping through elements of the ring\n",
        "        for node in subset:\n",
        "            time_from_terminal = k\n",
        "\n",
        "            # 1. Compute the feature vector representing (s, a)\n",
        "            x_sa = np.array([1 if i == node else 0 for i in range(env.n_states * env.n_actions)]).reshape(-1, 1)\n",
        "\n",
        "            # 2. Compute the q_hat_sa\n",
        "            q_hat_sa = np.dot(np.transpose(params), np.dot(successor_representation, x_sa))\n",
        "\n",
        "            # 3. Compute grad_q_hat_sa\n",
        "            grad_q_hat_sa = np.dot(successor_representation, x_sa)\n",
        "            \n",
        "            # 4. Compute the discounted return\n",
        "            discounted_return = np.power(discount_factor_wave, time_from_terminal) * reward\n",
        "\n",
        "            # 6. Compute update_vec\n",
        "            update_vec = learning_rate_wave * (discounted_return - q_hat_sa) * grad_q_hat_sa\n",
        "            dopamine[node] = (discounted_return - q_hat_sa).tolist()[0][0]\n",
        "\n",
        "            # 7. Parameter update\n",
        "            params += update_vec\n",
        "\n",
        "    return params, dopamine\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhpZpiN94Uqh"
      },
      "source": [
        "def compute_eps(episode_id, num_episodes, eps_min, expl_ratio):\n",
        "    expl_phase = expl_ratio * num_episodes\n",
        "    if episode_id < expl_phase:\n",
        "        eps = 1.0\n",
        "    else:\n",
        "        eps = np.power(eps_min, (episode_id - expl_phase) / (num_episodes - expl_phase))\n",
        "    \n",
        "    return eps"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yn0r0VRLZpc"
      },
      "source": [
        "# Dopamine waves visualization\n",
        "\n",
        "def visualize_waves(map, dopamine):\n",
        "    nodes_on_map = [j for i in map for j in i]\n",
        "\n",
        "    final_nodes = [int(nodes_on_map[0][0])]\n",
        "    final_edges = []\n",
        "\n",
        "    for element in nodes_on_map:\n",
        "        final_nodes.append(int(element[2]))\n",
        "        final_edges.append((int(element[0]), int(element[2])))\n",
        "\n",
        "    G = nx.Graph()\n",
        "    G.add_nodes_from(final_nodes)\n",
        "    G.add_edges_from(final_edges)\n",
        "\n",
        "    colors = []\n",
        "    for node in G.nodes():\n",
        "        color = dopamine[node]\n",
        "        colors.append(color)\n",
        "\n",
        "    cmap = plt.cm.get_cmap('rainbow')\n",
        "    cog_map = nx.draw_spring(G, node_color = colors, cmap = cmap, with_labels = True)\n",
        "    cbar = plt.cm.ScalarMappable(cmap=cmap)\n",
        "    plt.colorbar(cbar)\n",
        "    plt.axis(\"equal\")\n",
        "    plt.savefig(\"dopamine_waves\", dpi = 300, bbox_inches = \"tight\")\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvRkghQcrt0y"
      },
      "source": [
        "def sarsa(num_episodes, learning_rate, discount_factor, eps_min, expl_ratio, env_update, rand, in_labyrinth):\n",
        "    # Initialization\n",
        "    random.setstate(rand)\n",
        "    results = []\n",
        "\n",
        "    if in_labyrinth:\n",
        "        env = Maze(maze_labyrinth)\n",
        "    else:\n",
        "        env = Maze(maze_default)\n",
        "\n",
        "    start_state = env.start_state\n",
        "\n",
        "    Q = np.zeros((env.n_states, env.n_actions))\n",
        "\n",
        "    # SARSA algorithm\n",
        "    for i in range(num_episodes):\n",
        "\n",
        "        if i == int(np.around(env_update * num_episodes)):\n",
        "            env = Maze(maze_new_rew)\n",
        "\n",
        "        state = env.start_state\n",
        "\n",
        "        epsilon = compute_eps(i, num_episodes, eps_min, expl_ratio)\n",
        "\n",
        "        # Select an epsilon-greedy action\n",
        "        action = np.argmax(Q[state, :])\n",
        "        if (random.random() < epsilon) or (action not in env.possible_actions(state)):\n",
        "            action = random.choice(env.possible_actions(state))\n",
        "\n",
        "        while state != env.terminal_state:\n",
        "            reward = env.rewards[state, action]\n",
        "            next_state = env.move(state, action)\n",
        "\n",
        "            next_action = np.argmax(Q[next_state, :])\n",
        "            if (random.random() < epsilon) or (next_action not in env.possible_actions(next_state)):\n",
        "                next_action = random.choice(env.possible_actions(next_state))\n",
        "\n",
        "            Q[state, action] += learning_rate * (reward + discount_factor * Q[next_state, next_action] - Q[state, action])\n",
        "\n",
        "            if next_state == env.terminal_state:\n",
        "                # Obtain policy\n",
        "                policy = np.full(env.n_states, -1)\n",
        "                for s in range(env.n_states):\n",
        "                    policy[s] = np.argmax(Q[s, :])\n",
        "\n",
        "                # Extract the current performance between the starting state and the terminal state\n",
        "                _, res = env.simulate(start_state, policy)\n",
        "                results.append(res)\n",
        "\n",
        "            state = next_state\n",
        "            action = next_action\n",
        "\n",
        "    return Q, results\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QKcvSiqkI7A"
      },
      "source": [
        "def successor_representation_sarsa(num_episodes, learning_rate_base, discount_factor_base, eps_min, expl_ratio, env_update, rand, in_labyrinth):\n",
        "    # Initialization\n",
        "    random.setstate(rand)\n",
        "    results = []\n",
        "\n",
        "    if in_labyrinth:\n",
        "        env = Maze(maze_labyrinth)\n",
        "    else:\n",
        "        env = Maze(maze_default)\n",
        "\n",
        "    start_state = env.start_state\n",
        "\n",
        "    successor_representation = np.zeros((env.n_states * env.n_actions, env.n_states * env.n_actions))\n",
        "    reward_estimation = np.zeros(env.n_states * env.n_actions).reshape(-1, 1)\n",
        "\n",
        "    # \"Successor Representation - SARSA\" algorithm\n",
        "    for i in range(num_episodes):\n",
        "\n",
        "        if i == int(np.around(env_update * num_episodes)):\n",
        "            env = Maze(maze_new_rew)\n",
        "            reward_estimation = np.zeros(env.n_states * env.n_actions).reshape(-1, 1)\n",
        "\n",
        "        state = env.start_state\n",
        "\n",
        "        epsilon = compute_eps(i, num_episodes, eps_min, expl_ratio)\n",
        "\n",
        "        # Select an epsilon-greedy action\n",
        "        action = np.argmax(np.dot(successor_representation[state * env.n_actions : state * env.n_actions + env.n_actions, :], reward_estimation))\n",
        "        if (random.random() < epsilon) or (action not in env.possible_actions(state)):\n",
        "            action = random.choice(env.possible_actions(state))\n",
        "\n",
        "        while state != env.terminal_state:\n",
        "            reward = env.rewards[state, action]\n",
        "            next_state = env.move(state, action)\n",
        "\n",
        "            # Select next action with epsilon-greedy\n",
        "            next_action = np.argmax(np.dot(successor_representation[next_state * env.n_actions : next_state * env.n_actions + env.n_actions, :], reward_estimation))\n",
        "            if (random.random() < epsilon) or (next_action not in env.possible_actions(next_state)):\n",
        "                next_action = random.choice(env.possible_actions(next_state))\n",
        "\n",
        "            # Update Successor Representation\n",
        "            I_sa = np.array([1 if (i == state * env.n_actions + action) else 0 for i in range(env.n_states * env.n_actions)])\n",
        "            td_error = I_sa + discount_factor_base * successor_representation[next_state * env.n_actions + next_action, :] - successor_representation[state * env.n_actions + action, :]\n",
        "            successor_representation[state * env.n_actions + action, :] += learning_rate_base * td_error\n",
        "\n",
        "            # Update reward estimation\n",
        "            if next_state == env.terminal_state:\n",
        "                reward_estimation[state * env.n_actions + action] += learning_rate_base * (reward - reward_estimation[state * env.n_actions + action])\n",
        "\n",
        "                # Obtain policy\n",
        "                policy = np.full(env.n_states, -1)\n",
        "                for s in range(env.n_states):\n",
        "                    policy[s] = np.argmax(np.dot(successor_representation[s * env.n_actions : s * env.n_actions + env.n_actions, :], reward_estimation))\n",
        "            \n",
        "                # Extract the current performance between (0, 0) and the terminal state\n",
        "                _, res = env.simulate(start_state, policy)\n",
        "                results.append(res)\n",
        "\n",
        "            state = next_state\n",
        "            action = next_action\n",
        "\n",
        "    return successor_representation, reward_estimation, results\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2BxRsInQ_eo"
      },
      "source": [
        "def successor_representation_wave(num_episodes, learning_rate_base, discount_factor_base, eps_min, expl_ratio, learning_rate_wave, discount_factor_wave, env_update, rand, in_labyrinth):\n",
        "    # Initialization\n",
        "    random.setstate(rand)\n",
        "    results = []\n",
        "\n",
        "    if in_labyrinth:\n",
        "        env = Maze(maze_labyrinth)\n",
        "    else:\n",
        "        env = Maze(maze_default)\n",
        "\n",
        "    start_state = env.start_state\n",
        "\n",
        "    successor_representation = np.zeros((env.n_states * env.n_actions, env.n_states * env.n_actions))\n",
        "    weights = np.zeros(env.n_states * env.n_actions).reshape(-1, 1)\n",
        "\n",
        "    # \"Successor Representation - SARSA - wave-like\" algorithm\n",
        "    for i in range(num_episodes):\n",
        "        if i == int(np.around(env_update * num_episodes)):\n",
        "            env = Maze(maze_new_rew)\n",
        "            weights = np.zeros(env.n_states * env.n_actions).reshape(-1, 1)\n",
        "\n",
        "        state = env.start_state\n",
        "\n",
        "        epsilon = compute_eps(i, num_episodes, eps_min, expl_ratio)\n",
        "\n",
        "        # Select an epsilon-greedy action\n",
        "        action = np.argmax(np.dot(np.transpose(weights), successor_representation[:, state * env.n_actions : state * env.n_actions + env.n_actions]))\n",
        "        if (random.random() < epsilon) or (action not in env.possible_actions(state)):\n",
        "            action = random.choice(env.possible_actions(state))\n",
        "\n",
        "        while state != env.terminal_state:\n",
        "            reward = env.rewards[state, action]\n",
        "            next_state = env.move(state, action)\n",
        "\n",
        "            # Select next action with epsilon-greedy\n",
        "            next_action = np.argmax(np.dot(np.transpose(weights), successor_representation[:, next_state * env.n_actions : next_state * env.n_actions + env.n_actions]))\n",
        "            if (random.random() < epsilon) or (next_action not in env.possible_actions(next_state)):\n",
        "                next_action = random.choice(env.possible_actions(next_state))\n",
        "\n",
        "            # Update Successor Representation\n",
        "            I_sa = np.array([1 if (i == state * env.n_actions + action) else 0 for i in range(env.n_states * env.n_actions)])\n",
        "            td_error = I_sa + discount_factor_base * successor_representation[next_state * env.n_actions + next_action, :] - successor_representation[state * env.n_actions + action, :]\n",
        "            successor_representation[state * env.n_actions + action, :] += learning_rate_base * td_error\n",
        "\n",
        "            if next_state == env.terminal_state:\n",
        "                # Update reward estimation\n",
        "                target = state * env.n_actions + action\n",
        "                map = create_map(successor_representation, target, env)\n",
        "                weights, dop = update_value_weights_grad(env, successor_representation, weights, target, reward, map, learning_rate_wave, discount_factor_wave)\n",
        "\n",
        "                # if (i == int(np.around(0.4 * num_episodes))):\n",
        "                #     visualize_waves(map, dop)\n",
        "\n",
        "                # Obtain policy\n",
        "                policy = np.full(env.n_states, -1)\n",
        "                for s in range(env.n_states):\n",
        "                    policy[s] = np.argmax(np.dot(np.transpose(weights), successor_representation[:, s * env.n_actions : s * env.n_actions + env.n_actions]))\n",
        "            \n",
        "                # Extract the current performance between (0, 0) and the terminal state\n",
        "                _, res = env.simulate(start_state, policy)\n",
        "                results.append(res)\n",
        "\n",
        "            state = next_state\n",
        "            action = next_action\n",
        "\n",
        "    return successor_representation, weights, map, dop, results\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbmjVBMfgJdf"
      },
      "source": [
        "def sarsa_lambda(num_episodes, learning_rate, discount_factor, eps_min, expl_ratio, trace_decay, env_update, rand, in_labyrinth):\n",
        "    # Initialization\n",
        "    random.setstate(rand)\n",
        "    results = []\n",
        "\n",
        "    if in_labyrinth:\n",
        "        env = Maze(maze_labyrinth)\n",
        "    else:\n",
        "        env = Maze(maze_default)\n",
        "\n",
        "    start_state = env.start_state\n",
        "    weights     = np.zeros(env.n_states * env.n_actions).reshape(-1, 1)\n",
        "\n",
        "    # SARSA(lambda) algorithm\n",
        "    for i in range(num_episodes):\n",
        "\n",
        "        if i == int(np.around(env_update * num_episodes)):\n",
        "            env = Maze(maze_new_rew)\n",
        "\n",
        "        state = env.start_state\n",
        "\n",
        "        epsilon = compute_eps(i, num_episodes, eps_min, expl_ratio)\n",
        "\n",
        "        # Select an epsilon-greedy action\n",
        "        action = np.argmax(weights[state * env.n_actions : state * env.n_actions + env.n_actions])\n",
        "        if (random.random() < epsilon) or (action not in env.possible_actions(state)):\n",
        "            action = random.choice(env.possible_actions(state))\n",
        "\n",
        "        # Generate one-hot feature vector\n",
        "        feature = np.zeros(env.n_states * env.n_actions).reshape(-1, 1)\n",
        "        feature[state * env.n_actions + action] = 1\n",
        "\n",
        "        eligibility_trace = np.zeros(env.n_states * env.n_actions).reshape(-1, 1)\n",
        "\n",
        "        while state != env.terminal_state:\n",
        "            reward = env.rewards[state, action]\n",
        "            next_state = env.move(state, action)\n",
        "\n",
        "            eligibility_trace = discount_factor * trace_decay * eligibility_trace + feature\n",
        "\n",
        "            next_action = np.argmax(weights[next_state * env.n_actions : next_state * env.n_actions + env.n_actions])\n",
        "            if (random.random() < epsilon) or (next_action not in env.possible_actions(next_state)):\n",
        "                next_action = random.choice(env.possible_actions(next_state))\n",
        "\n",
        "            next_feature = np.zeros(env.n_states * env.n_actions).reshape(-1, 1)\n",
        "            next_feature[next_state * env.n_actions + next_action] = 1\n",
        "\n",
        "            reward_prediction_error = reward + discount_factor * np.dot(np.transpose(weights), next_feature) - np.dot(np.transpose(weights), feature)\n",
        "\n",
        "            weights += learning_rate * reward_prediction_error * eligibility_trace\n",
        "\n",
        "            if next_state == env.terminal_state:\n",
        "                # Obtain policy\n",
        "                policy = [np.argmax(a) for a in [weights[s : s + env.n_actions] for s in range(0, len(weights), env.n_actions)]]\n",
        "\n",
        "                # Extract the current performance between (0, 0) and the terminal state\n",
        "                _, res = env.simulate(start_state, policy)\n",
        "                results.append(res)\n",
        "\n",
        "            feature = next_feature\n",
        "            state = next_state\n",
        "            action = next_action\n",
        "\n",
        "    return weights, results\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTVJJdoKTgam"
      },
      "source": [
        "# ------------------------------------------------------------------------------"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKbjFajDZR5R"
      },
      "source": [
        "# Benchmarking simple environment\n",
        "# a) Speed\n",
        "# b) Reliability\n",
        "# c) Flexibility\n",
        "\n",
        "env = Maze(maze_default)\n",
        "\n",
        "num_episodes = 40\n",
        "num_test_cases = 10\n",
        "num_perturbations = 100\n",
        "\n",
        "env_update = 1.1\n",
        "run_in_labyrinth = False\n",
        "\n",
        "# Hyperparameters\n",
        "\n",
        "# 1) Base\n",
        "lr_base = 0.05\n",
        "df_base = 0.99\n",
        "eps_min = 0.05\n",
        "expl_ratio = 1.0\n",
        "\n",
        "# 2) Wave-like\n",
        "lr_wave = 0.5\n",
        "df_wave = 0.5\n",
        "\n",
        "# 3) SARSA(lambda)\n",
        "trace_decay = 0.9\n",
        "\n",
        "# Initialization\n",
        "# 0: SARSA Q-learning, 1: SR, 2: Wave, 3: Eligibility trace\n",
        "# 4 algorithms being compared, first row: when was the earliest step an optimal trajectory was found, second row: which algorithm\n",
        "speed_test = np.zeros((num_test_cases * 4, 2))\n",
        "\n",
        "# 4 algorithms being compared, first row: how many times did the agent found the reward, second row: which algorithm\n",
        "reliability_test = np.zeros((num_test_cases * 4, 2))\n",
        "\n",
        "flexibility_test = []\n",
        "\n",
        "for i in range(num_test_cases):\n",
        "    rand_state = random.getstate()\n",
        "\n",
        "    Q_sarsa, fin_res_sarsa = sarsa(num_episodes, lr_base, df_base, eps_min, expl_ratio, env_update, rand_state, run_in_labyrinth)\n",
        "    speed_test[i * 4, 0] = np.argmin(fin_res_sarsa)\n",
        "    speed_test[i * 4, 1] = 0\n",
        "    fin_res_sarsa = np.array([0 if (i == np.inf) else i for i in fin_res_sarsa])\n",
        "    reliability_test[i * 4, 0] = np.count_nonzero(fin_res_sarsa) / num_episodes\n",
        "    reliability_test[i * 4, 1] = 0\n",
        "\n",
        "    succ_rep_base, rew_est_base, fin_res_base = successor_representation_sarsa(num_episodes, lr_base, df_base, eps_min, expl_ratio, env_update, rand_state, run_in_labyrinth)\n",
        "    speed_test[(i * 4) + 1, 0] = np.argmin(fin_res_base)\n",
        "    speed_test[(i * 4) + 1, 1] = 1\n",
        "    fin_res_base = np.array([0 if (i == np.inf) else i for i in fin_res_base])\n",
        "    reliability_test[(i * 4) + 1, 0] = np.count_nonzero(fin_res_base) / num_episodes\n",
        "    reliability_test[(i * 4) + 1, 1] = 1\n",
        "\n",
        "    succ_rep_wave, weights_wave, map_final, dop, fin_res_wave = successor_representation_wave(num_episodes, lr_base, df_base, eps_min, expl_ratio, lr_wave, df_wave, env_update, rand_state, run_in_labyrinth)\n",
        "    speed_test[(i * 4) + 2, 0] = np.argmin(fin_res_wave)\n",
        "    speed_test[(i * 4) + 2, 1] = 2\n",
        "    fin_res_wave = np.array([0 if (i == np.inf) else i for i in fin_res_wave])\n",
        "    reliability_test[(i * 4) + 2, 0] = np.count_nonzero(fin_res_wave) / num_episodes\n",
        "    reliability_test[(i * 4) + 2, 1] = 2\n",
        "\n",
        "    w, fin_res_lambda = sarsa_lambda(num_episodes, lr_base, df_base, eps_min, expl_ratio, trace_decay, env_update, rand_state, run_in_labyrinth)\n",
        "    speed_test[(i * 4) + 3, 0] = np.argmin(fin_res_lambda)\n",
        "    speed_test[(i * 4) + 3, 1] = 3\n",
        "    fin_res_lambda = np.array([0 if (i == np.inf) else i for i in fin_res_lambda])\n",
        "    reliability_test[(i * 4) + 3, 0] = np.count_nonzero(fin_res_lambda) / num_episodes\n",
        "    reliability_test[(i * 4) + 3, 1] = 3\n",
        "\n",
        "    # Flexibility test looping through rows of the maze\n",
        "    for r in range(env.maze.shape[0]):\n",
        "        distracted_states = []\n",
        "        for c in range(env.maze.shape[1]):\n",
        "            distracted_states.append(env.map[(r, c)])\n",
        "        distraction_level = 2\n",
        "\n",
        "        # SARSA\n",
        "        policy_sarsa = np.full(env.n_states, -1)\n",
        "        for state in range(env.n_states):\n",
        "            if state in distracted_states:\n",
        "                policy_sarsa[state] = np.argsort(Q_sarsa[state, :])[-distraction_level]\n",
        "            else:\n",
        "                policy_sarsa[state] = np.argsort(Q_sarsa[state, :])[-1]\n",
        "\n",
        "        _, pert_sarsa = env.simulate(env.start_state, policy_sarsa)\n",
        "        if pert_sarsa == np.inf:\n",
        "            success = 0\n",
        "        else:\n",
        "            success = 1\n",
        "\n",
        "        # Successor Representation\n",
        "        policy_sr = np.full(env.n_states, -1)\n",
        "        for state in range(env.n_states):\n",
        "            if state in distracted_states:\n",
        "                policy_sr[state] = np.argsort(np.dot(succ_rep_base[state * env.n_actions : state * env.n_actions + env.n_actions, :], rew_est_base).flatten())[-distraction_level]\n",
        "            else:\n",
        "                policy_sr[state] = np.argsort(np.dot(succ_rep_base[state * env.n_actions : state * env.n_actions + env.n_actions, :], rew_est_base).flatten())[-1]\n",
        "\n",
        "        _, pert_sr = env.simulate(env.start_state, policy_sr)\n",
        "        if pert_sr == np.inf:\n",
        "            success = 0\n",
        "        else:\n",
        "            success = 1\n",
        "\n",
        "        # Wave-like\n",
        "        policy_wave = np.full(env.n_states, -1)\n",
        "        for state in range(env.n_states):\n",
        "            if state in distracted_states:\n",
        "                policy_wave[state] = np.argsort(np.dot(np.transpose(weights_wave), succ_rep_wave[:, state * env.n_actions : state * env.n_actions + env.n_actions]).flatten())[-distraction_level]\n",
        "            else:\n",
        "                policy_wave[state] = np.argsort(np.dot(np.transpose(weights_wave), succ_rep_wave[:, state * env.n_actions : state * env.n_actions + env.n_actions]).flatten())[-1]\n",
        "\n",
        "        _, pert_wave = env.simulate(env.start_state, policy_wave)\n",
        "        if pert_wave == np.inf:\n",
        "            success = 0\n",
        "        else:\n",
        "            success = 1\n",
        "\n",
        "        # Eligibility traces\n",
        "        policy_lambda = np.full(env.n_states, -1)\n",
        "        w = w.flatten()\n",
        "        for state in range(env.n_states):\n",
        "            if state in distracted_states:\n",
        "                policy_lambda[state] = np.argsort(w[state * env.n_actions : state * env.n_actions + env.n_actions])[-distraction_level]\n",
        "            else:\n",
        "                policy_lambda[state] = np.argsort(w[state * env.n_actions : state * env.n_actions + env.n_actions])[-1]\n",
        "\n",
        "        _, pert_lambda = env.simulate(env.start_state, policy_lambda)\n",
        "        if pert_lambda == np.inf:\n",
        "            success = 0\n",
        "        else:\n",
        "            success = 1\n",
        "\n",
        "    # Flexibility test looping through columns of the maze\n",
        "    for c in range(env.maze.shape[1]):\n",
        "        distracted_states = []\n",
        "        for r in range(env.maze.shape[0]):\n",
        "            distracted_states.append(env.map[(r, c)])\n",
        "        distraction_level = 2\n",
        "\n",
        "        # SARSA\n",
        "        policy_sarsa = np.full(env.n_states, -1)\n",
        "        for state in range(env.n_states):\n",
        "            if state in distracted_states:\n",
        "                policy_sarsa[state] = np.argsort(Q_sarsa[state, :])[-distraction_level]\n",
        "            else:\n",
        "                policy_sarsa[state] = np.argsort(Q_sarsa[state, :])[-1]\n",
        "\n",
        "        _, pert_sarsa = env.simulate(env.start_state, policy_sarsa)\n",
        "        if pert_sarsa == np.inf:\n",
        "            success = 0\n",
        "        else:\n",
        "            success = 1\n",
        "\n",
        "        # Successor Representation\n",
        "        policy_sr = np.full(env.n_states, -1)\n",
        "        for state in range(env.n_states):\n",
        "            if state in distracted_states:\n",
        "                policy_sr[state] = np.argsort(np.dot(succ_rep_base[state * env.n_actions : state * env.n_actions + env.n_actions, :], rew_est_base).flatten())[-distraction_level]\n",
        "            else:\n",
        "                policy_sr[state] = np.argsort(np.dot(succ_rep_base[state * env.n_actions : state * env.n_actions + env.n_actions, :], rew_est_base).flatten())[-1]\n",
        "\n",
        "        _, pert_sr = env.simulate(env.start_state, policy_sr)\n",
        "        if pert_sr == np.inf:\n",
        "            success = 0\n",
        "        else:\n",
        "            success = 1\n",
        "\n",
        "        # Wave-like\n",
        "        policy_wave = np.full(env.n_states, -1)\n",
        "        for state in range(env.n_states):\n",
        "            if state in distracted_states:\n",
        "                policy_wave[state] = np.argsort(np.dot(np.transpose(weights_wave), succ_rep_wave[:, state * env.n_actions : state * env.n_actions + env.n_actions]).flatten())[-distraction_level]\n",
        "            else:\n",
        "                policy_wave[state] = np.argsort(np.dot(np.transpose(weights_wave), succ_rep_wave[:, state * env.n_actions : state * env.n_actions + env.n_actions]).flatten())[-1]\n",
        "\n",
        "        _, pert_wave = env.simulate(env.start_state, policy_wave)\n",
        "        if pert_wave == np.inf:\n",
        "            success = 0\n",
        "        else:\n",
        "            success = 1\n",
        "\n",
        "        # Eligibility traces\n",
        "        policy_lambda = np.full(env.n_states, -1)\n",
        "        w = w.flatten()\n",
        "        for state in range(env.n_states):\n",
        "            if state in distracted_states:\n",
        "                policy_lambda[state] = np.argsort(w[state * env.n_actions : state * env.n_actions + env.n_actions])[-distraction_level]\n",
        "            else:\n",
        "                policy_lambda[state] = np.argsort(w[state * env.n_actions : state * env.n_actions + env.n_actions])[-1]\n",
        "\n",
        "        _, pert_lambda = env.simulate(env.start_state, policy_lambda)\n",
        "        if pert_lambda == np.inf:\n",
        "            success = 0\n",
        "        else:\n",
        "            success = 1\n",
        "\n",
        "\n",
        "    # Flexibility test randomly selected state perturbations\n",
        "    for j in range(num_perturbations):\n",
        "        for ind, k in enumerate([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]):\n",
        "            distracted_states = random.sample([s for s in range(env.n_states) if s != env.terminal_state], k = int(np.floor(env.n_states * k)))\n",
        "            distraction_level = 2\n",
        "\n",
        "            # SARSA\n",
        "            policy_sarsa = np.full(env.n_states, -1)\n",
        "            for state in range(env.n_states):\n",
        "                if state in distracted_states:\n",
        "                    policy_sarsa[state] = np.argsort(Q_sarsa[state, :])[-distraction_level]\n",
        "                else:\n",
        "                    policy_sarsa[state] = np.argsort(Q_sarsa[state, :])[-1]\n",
        "\n",
        "            _, pert_sarsa = env.simulate(env.start_state, policy_sarsa)\n",
        "            if pert_sarsa == np.inf:\n",
        "                success = 0\n",
        "            else:\n",
        "                success = 1\n",
        "            flexibility_test.append([success, 0, k])\n",
        "\n",
        "            # Successor Representation\n",
        "            policy_sr = np.full(env.n_states, -1)\n",
        "            for state in range(env.n_states):\n",
        "                if state in distracted_states:\n",
        "                    policy_sr[state] = np.argsort(np.dot(succ_rep_base[state * env.n_actions : state * env.n_actions + env.n_actions, :], rew_est_base).flatten())[-distraction_level]\n",
        "                else:\n",
        "                    policy_sr[state] = np.argsort(np.dot(succ_rep_base[state * env.n_actions : state * env.n_actions + env.n_actions, :], rew_est_base).flatten())[-1]\n",
        "\n",
        "            _, pert_sr = env.simulate(env.start_state, policy_sr)\n",
        "            if pert_sr == np.inf:\n",
        "                success = 0\n",
        "            else:\n",
        "                success = 1\n",
        "            flexibility_test.append([success, 1, k])\n",
        "\n",
        "\n",
        "            # Wave-like\n",
        "            policy_wave = np.full(env.n_states, -1)\n",
        "            for state in range(env.n_states):\n",
        "                if state in distracted_states:\n",
        "                    policy_wave[state] = np.argsort(np.dot(np.transpose(weights_wave), succ_rep_wave[:, state * env.n_actions : state * env.n_actions + env.n_actions]).flatten())[-distraction_level]\n",
        "                else:\n",
        "                    policy_wave[state] = np.argsort(np.dot(np.transpose(weights_wave), succ_rep_wave[:, state * env.n_actions : state * env.n_actions + env.n_actions]).flatten())[-1]\n",
        "\n",
        "            _, pert_wave = env.simulate(env.start_state, policy_wave)\n",
        "            if pert_wave == np.inf:\n",
        "                success = 0\n",
        "            else:\n",
        "                success = 1\n",
        "            flexibility_test.append([success, 2, k])\n",
        "\n",
        "\n",
        "            # Eligibility traces\n",
        "            policy_lambda = np.full(env.n_states, -1)\n",
        "            w = w.flatten()\n",
        "            for state in range(env.n_states):\n",
        "                if state in distracted_states:\n",
        "                    policy_lambda[state] = np.argsort(w[state * env.n_actions : state * env.n_actions + env.n_actions])[-distraction_level]\n",
        "                else:\n",
        "                    policy_lambda[state] = np.argsort(w[state * env.n_actions : state * env.n_actions + env.n_actions])[-1]\n",
        "\n",
        "            _, pert_lambda = env.simulate(env.start_state, policy_lambda)\n",
        "            if pert_lambda == np.inf:\n",
        "                success = 0\n",
        "            else:\n",
        "                success = 1\n",
        "            flexibility_test.append([success, 3, k])\n",
        "\n",
        "    print(i)\n",
        "\n",
        "flexibility_test = np.array(flexibility_test)\n",
        "\n",
        "sns.color_palette(\"colorblind\")\n",
        "# Plotting the results\n",
        "# 1. Speed\n",
        "df_speed_test = pd.DataFrame(speed_test, columns = [\"first_reached\", \"algorithms\"])\n",
        "df_speed_test = df_speed_test.astype('int32')\n",
        "give_names = {0: \"SARSA\", 1: \"Original SR\", 2: \"Wave-like SR\", 3: \"SARSA($\\lambda$)\"}\n",
        "df_speed_test = df_speed_test.replace({\"algorithms\": give_names})\n",
        "sns.set_theme(style = \"white\")\n",
        "sns_speed_test = sns.catplot(x = \"algorithms\", y = \"first_reached\", kind = \"violin\", data = df_speed_test)\n",
        "sns_speed_test.set(ylim = (0, None))\n",
        "sns_speed_test.set(xlabel=\"Algorithms\", ylabel = \"First attempt leading to the reward\")\n",
        "sns_speed_test.savefig(\"speed_test_simple\", dpi = 300)\n",
        "\n",
        "# 2. Reliability\n",
        "df_reliability_test = pd.DataFrame(reliability_test, columns = [\"ratio_reward_obtained\", \"algorithms\"])\n",
        "give_names = {0: \"SARSA\", 1: \"Original SR\", 2: \"Wave-like SR\", 3: \"SARSA($\\lambda$)\"}\n",
        "df_reliability_test = df_reliability_test.replace({\"algorithms\": give_names})\n",
        "sns.set_theme(style = \"white\")\n",
        "sns_reliability_test = sns.catplot(x = \"algorithms\", y = \"ratio_reward_obtained\", kind = \"violin\", data = df_reliability_test)\n",
        "sns_reliability_test.set(xlabel=\"Algorithms\", ylabel = \"Ratio of the attempts leading to the reward\")\n",
        "sns_reliability_test.set(ylim = (0.0, 1.0))\n",
        "sns_reliability_test.savefig(\"reliability_test_simple\", dpi = 300)\n",
        "\n",
        "# 3. Flexibility\n",
        "df_flexibility_test = pd.DataFrame(flexibility_test, columns = [\"ratio_reward_obtained\", \"algorithms\", \"pert_level\"])\n",
        "give_names = {0: \"SARSA\", 1: \"Original SR\", 2: \"Wave-like SR\", 3: \"SARSA($\\lambda$)\"}\n",
        "pert_names = {0.1: \"10 %\", 0.2: \"20 %\", 0.3: \"30 %\", 0.4: \"40 %\", 0.5: \"50 %\", 0.6: \"60 %\", 0.7: \"70 %\", 0.8: \"80 %\"}\n",
        "df_flexibility_test = df_flexibility_test.replace({\"algorithms\": give_names})\n",
        "df_flexibility_test = df_flexibility_test.replace({\"pert_level\": pert_names})\n",
        "sns.set_theme(style = \"darkgrid\")\n",
        "sns_flexibility_test = sns.relplot(x = \"pert_level\", y = \"ratio_reward_obtained\", hue = \"algorithms\", kind = \"line\", data = df_flexibility_test)\n",
        "sns_flexibility_test.set(xlabel=\"Ratio of perturbed states\", ylabel = \"Ratio of the attempts leading to the reward\")\n",
        "sns_flexibility_test.savefig(\"flexibility_test_simple\", dpi = 300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKu7FM_reuTp"
      },
      "source": [
        "# Benchmarking for change in reward location\n",
        "# a) Flexibility\n",
        "\n",
        "env = Maze(maze_new_rew)\n",
        "\n",
        "num_episodes = 40\n",
        "num_test_cases = 10\n",
        "num_perturbations = 100\n",
        "\n",
        "env_update = 0.875\n",
        "run_in_labyrinth = False\n",
        "\n",
        "# Hyperparameters\n",
        "\n",
        "# 1) Base\n",
        "lr_base = 0.05\n",
        "df_base = 0.99\n",
        "eps_min = 0.05\n",
        "expl_ratio = 1.0\n",
        "\n",
        "# 2) Wave-like\n",
        "lr_wave = 0.5\n",
        "df_wave = 0.5\n",
        "\n",
        "# 3) SARSA(lambda)\n",
        "trace_decay = 0.9\n",
        "\n",
        "flexibility_test = []\n",
        "\n",
        "for i in range(num_test_cases):\n",
        "    rand_state = random.getstate()\n",
        "\n",
        "    Q_sarsa, fin_res_sarsa = sarsa(num_episodes, lr_base, df_base, eps_min, expl_ratio, env_update, rand_state, run_in_labyrinth)\n",
        "\n",
        "    succ_rep_base, rew_est_base, fin_res_base = successor_representation_sarsa(num_episodes, lr_base, df_base, eps_min, expl_ratio, env_update, rand_state, run_in_labyrinth)\n",
        "\n",
        "    succ_rep_wave, weights_wave, map_final, dop, fin_res_wave = successor_representation_wave(num_episodes, lr_base, df_base, eps_min, expl_ratio, lr_wave, df_wave, env_update, rand_state, run_in_labyrinth)\n",
        "\n",
        "    w, fin_res_lambda = sarsa_lambda(num_episodes, lr_base, df_base, eps_min, expl_ratio, trace_decay, env_update, rand_state, run_in_labyrinth)\n",
        "\n",
        "    # Flexibility test, randomly selected states being distracted\n",
        "    for j in range(num_perturbations):\n",
        "        for ind, k in enumerate([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]):\n",
        "            distracted_states = random.sample([s for s in range(env.n_states) if s != env.terminal_state], k = int(np.floor(env.n_states * k)))\n",
        "            distraction_level = 2\n",
        "\n",
        "            # SARSA\n",
        "            policy_sarsa = np.full(env.n_states, -1)\n",
        "            for state in range(env.n_states):\n",
        "                if state in distracted_states:\n",
        "                    policy_sarsa[state] = np.argsort(Q_sarsa[state, :])[-distraction_level]\n",
        "                else:\n",
        "                    policy_sarsa[state] = np.argsort(Q_sarsa[state, :])[-1]\n",
        "\n",
        "            _, pert_sarsa = env.simulate(env.start_state, policy_sarsa)\n",
        "            if pert_sarsa == np.inf:\n",
        "                success = 0\n",
        "            else:\n",
        "                success = 1\n",
        "            flexibility_test.append([success, 0, k])\n",
        "\n",
        "            # Successor Representation\n",
        "            policy_sr = np.full(env.n_states, -1)\n",
        "            for state in range(env.n_states):\n",
        "                if state in distracted_states:\n",
        "                    policy_sr[state] = np.argsort(np.dot(succ_rep_base[state * env.n_actions : state * env.n_actions + env.n_actions, :], rew_est_base).flatten())[-distraction_level]\n",
        "                else:\n",
        "                    policy_sr[state] = np.argsort(np.dot(succ_rep_base[state * env.n_actions : state * env.n_actions + env.n_actions, :], rew_est_base).flatten())[-1]\n",
        "\n",
        "            _, pert_sr = env.simulate(env.start_state, policy_sr)\n",
        "            if pert_sr == np.inf:\n",
        "                success = 0\n",
        "            else:\n",
        "                success = 1\n",
        "            flexibility_test.append([success, 1, k])\n",
        "\n",
        "\n",
        "            # Wave-like\n",
        "            policy_wave = np.full(env.n_states, -1)\n",
        "            for state in range(env.n_states):\n",
        "                if state in distracted_states:\n",
        "                    policy_wave[state] = np.argsort(np.dot(np.transpose(weights_wave), succ_rep_wave[:, state * env.n_actions : state * env.n_actions + env.n_actions]).flatten())[-distraction_level]\n",
        "                else:\n",
        "                    policy_wave[state] = np.argsort(np.dot(np.transpose(weights_wave), succ_rep_wave[:, state * env.n_actions : state * env.n_actions + env.n_actions]).flatten())[-1]\n",
        "\n",
        "            _, pert_wave = env.simulate(env.start_state, policy_wave)\n",
        "            if pert_wave == np.inf:\n",
        "                success = 0\n",
        "            else:\n",
        "                success = 1\n",
        "            flexibility_test.append([success, 2, k])\n",
        "\n",
        "\n",
        "            # Eligibility traces\n",
        "            policy_lambda = np.full(env.n_states, -1)\n",
        "            w = w.flatten()\n",
        "            for state in range(env.n_states):\n",
        "                if state in distracted_states:\n",
        "                    policy_lambda[state] = np.argsort(w[state * env.n_actions : state * env.n_actions + env.n_actions])[-distraction_level]\n",
        "                else:\n",
        "                    policy_lambda[state] = np.argsort(w[state * env.n_actions : state * env.n_actions + env.n_actions])[-1]\n",
        "\n",
        "            _, pert_lambda = env.simulate(env.start_state, policy_lambda)\n",
        "            if pert_lambda == np.inf:\n",
        "                success = 0\n",
        "            else:\n",
        "                success = 1\n",
        "            flexibility_test.append([success, 3, k])\n",
        "\n",
        "    print(i)\n",
        "\n",
        "flexibility_test = np.array(flexibility_test)\n",
        "\n",
        "sns.color_palette(\"colorblind\")\n",
        "# Plotting the results\n",
        "# 1. Flexibility\n",
        "df_flexibility_test = pd.DataFrame(flexibility_test, columns = [\"ratio_reward_obtained\", \"algorithms\", \"pert_level\"])\n",
        "give_names = {0: \"SARSA\", 1: \"Original SR\", 2: \"Wave-like SR\", 3: \"SARSA($\\lambda$)\"}\n",
        "pert_names = {0.1: \"10 %\", 0.2: \"20 %\", 0.3: \"30 %\", 0.4: \"40 %\", 0.5: \"50 %\", 0.6: \"60 %\", 0.7: \"70 %\", 0.8: \"80 %\"}\n",
        "df_flexibility_test = df_flexibility_test.replace({\"algorithms\": give_names})\n",
        "df_flexibility_test = df_flexibility_test.replace({\"pert_level\": pert_names})\n",
        "sns.set_theme(style = \"darkgrid\")\n",
        "sns_flexibility_test = sns.relplot(x = \"pert_level\", y = \"ratio_reward_obtained\", hue = \"algorithms\", kind = \"line\", data = df_flexibility_test)\n",
        "sns_flexibility_test.set(xlabel=\"Ratio of perturbed states\", ylabel = \"Ratio of the attempts leading to the reward\")\n",
        "sns_flexibility_test.savefig(\"flexibility_test_change_rew\", dpi = 300)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSElgkXUfRq7"
      },
      "source": [
        "# Benchmarking in the labyrinth\n",
        "\n",
        "# a) Speed\n",
        "# b) Reliability\n",
        "\n",
        "num_episodes = 20\n",
        "num_test_cases = 10\n",
        "\n",
        "env_update = 1.1\n",
        "run_in_labyrinth = True\n",
        "\n",
        "# Hyperparameters\n",
        "\n",
        "# 1) Base\n",
        "lr_base = 0.05\n",
        "df_base = 0.99\n",
        "eps_min = 0.05\n",
        "expl_ratio = 1.0\n",
        "\n",
        "# 2) Wave-like\n",
        "lr_wave = 0.5\n",
        "df_wave = 0.6\n",
        "\n",
        "# 3) SARSA(lambda)\n",
        "trace_decay = 0.9\n",
        "\n",
        "# Initialization\n",
        "# 0: SARSA Q-learning, 1: SR, 2: Wave, 3: Eligibility trace\n",
        "# 4 algorithms being compared, first row: when was the earliest step an optimal trajectory was found, second row: which algorithm\n",
        "speed_test = np.zeros((num_test_cases * 4, 2))\n",
        "\n",
        "# 4 algorithms being compared, first row: how many times did the agent found the reward, second row: which algorithm\n",
        "reliability_test = np.zeros((num_test_cases * 4, 2))\n",
        "\n",
        "for i in range(num_test_cases):\n",
        "    rand_state = random.getstate()\n",
        "\n",
        "    Q_sarsa, fin_res_sarsa = sarsa(num_episodes, lr_base, df_base, eps_min, expl_ratio, env_update, rand_state, run_in_labyrinth)\n",
        "    speed_test[i * 4, 0] = np.argmin(fin_res_sarsa)\n",
        "    speed_test[i * 4, 1] = 0\n",
        "    fin_res_sarsa = np.array([0 if (i == np.inf) else i for i in fin_res_sarsa])\n",
        "    reliability_test[i * 4, 0] = np.count_nonzero(fin_res_sarsa) / num_episodes\n",
        "    reliability_test[i * 4, 1] = 0\n",
        "\n",
        "    succ_rep_base, rew_est_base, fin_res_base = successor_representation_sarsa(num_episodes, lr_base, df_base, eps_min, expl_ratio, env_update, rand_state, run_in_labyrinth)\n",
        "    speed_test[(i * 4) + 1, 0] = np.argmin(fin_res_base)\n",
        "    speed_test[(i * 4) + 1, 1] = 1\n",
        "    fin_res_base = np.array([0 if (i == np.inf) else i for i in fin_res_base])\n",
        "    reliability_test[(i * 4) + 1, 0] = np.count_nonzero(fin_res_base) / num_episodes\n",
        "    reliability_test[(i * 4) + 1, 1] = 1\n",
        "\n",
        "    succ_rep_wave, weights_wave, map_final, dop, fin_res_wave = successor_representation_wave(num_episodes, lr_base, df_base, eps_min, expl_ratio, lr_wave, df_wave, env_update, rand_state, run_in_labyrinth)\n",
        "    speed_test[(i * 4) + 2, 0] = np.argmin(fin_res_wave)\n",
        "    speed_test[(i * 4) + 2, 1] = 2\n",
        "    fin_res_wave = np.array([0 if (i == np.inf) else i for i in fin_res_wave])\n",
        "    reliability_test[(i * 4) + 2, 0] = np.count_nonzero(fin_res_wave) / num_episodes\n",
        "    reliability_test[(i * 4) + 2, 1] = 2\n",
        "\n",
        "    w, fin_res_lambda = sarsa_lambda(num_episodes, lr_base, df_base, eps_min, expl_ratio, trace_decay, env_update, rand_state, run_in_labyrinth)\n",
        "    speed_test[(i * 4) + 3, 0] = np.argmin(fin_res_lambda)\n",
        "    speed_test[(i * 4) + 3, 1] = 3\n",
        "    fin_res_lambda = np.array([0 if (i == np.inf) else i for i in fin_res_lambda])\n",
        "    reliability_test[(i * 4) + 3, 0] = np.count_nonzero(fin_res_lambda) / num_episodes\n",
        "    reliability_test[(i * 4) + 3, 1] = 3\n",
        "\n",
        "    print(i)\n",
        "\n",
        "sns.color_palette(\"colorblind\")\n",
        "# Plotting the results\n",
        "# 1. Speed\n",
        "df_speed_test = pd.DataFrame(speed_test, columns = [\"first_reached\", \"algorithms\"])\n",
        "df_speed_test = df_speed_test.astype('int32')\n",
        "give_names = {0: \"SARSA\", 1: \"Original SR\", 2: \"Wave-like SR\", 3: \"SARSA($\\lambda$)\"}\n",
        "df_speed_test = df_speed_test.replace({\"algorithms\": give_names})\n",
        "sns.set_theme(style = \"white\")\n",
        "sns_speed_test = sns.catplot(x = \"algorithms\", y = \"first_reached\", kind = \"violin\", data = df_speed_test)\n",
        "sns_speed_test.set(ylim = (0, None))\n",
        "sns_speed_test.set(xlabel=\"Algorithms\", ylabel = \"First attempt leading to the reward\")\n",
        "sns_speed_test.savefig(\"speed_test_labyrinth\", dpi = 300)\n",
        "\n",
        "# 2. Reliability\n",
        "df_reliability_test = pd.DataFrame(reliability_test, columns = [\"ratio_reward_obtained\", \"algorithms\"])\n",
        "give_names = {0: \"SARSA\", 1: \"Original SR\", 2: \"Wave-like SR\", 3: \"SARSA($\\lambda$)\"}\n",
        "df_reliability_test = df_reliability_test.replace({\"algorithms\": give_names})\n",
        "sns.set_theme(style = \"white\")\n",
        "sns_reliability_test = sns.catplot(x = \"algorithms\", y = \"ratio_reward_obtained\", kind = \"violin\", data = df_reliability_test)\n",
        "sns_reliability_test.set(xlabel=\"Algorithms\", ylabel = \"Ratio of the attempts leading to the reward\")\n",
        "sns_reliability_test.set(ylim = (0.0, 1.0))\n",
        "sns_reliability_test.savefig(\"reliability_test_labyrinth\", dpi = 300)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86Fhtxy6PmEP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}